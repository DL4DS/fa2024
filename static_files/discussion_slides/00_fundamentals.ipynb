{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/DL4DS/sp2024_notebooks/blob/main/discussion/00_fundamentals.ipynb\" target=\"_blank\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# 00. Fundamentals of PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.2'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# imports\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal of this notebook\n",
    "- Introduction to PyTorch\n",
    "- Understanding Tensors\n",
    "- Fundamental Tensor Operations and Methods\n",
    "- Few Questions to test your understanding"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brief Introduction to PyTorch\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Origin Story\n",
    "\n",
    "Picture the early 2000s, when there was a need to develop ML frameworks that were flexible enough for daily research tasks. Out of this need, the Torch framework was born. Torch was released in 2002 out of the efforts of Ronan Collobert, Koray Kavukcuoglu, and Clement Farabet. It was later picked up by Facebook AI Research and many other people from several universities and research groups. Although Torch did give the required flexibility, it was written in Lua, which was not a very popular language, and the major drawback the community faced was the learning curve to the new language. \n",
    "\n",
    "The widespread acceptance of Pyhon in the ML community made the developers pivot to Python, and thus the Python based version of Torch a.k.a PyTorch was born. PyTorch began as an internship project by Adam Paszke, who was working under Soumith Chintala, a core developer of Torch.\n",
    "\n",
    "Ref: https://subscription.packtpub.com/book/data/9781788834131/1/ch01lvl1sec02/understanding-pytorch-s-history"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In a nutshell\n",
    "\n",
    "PyTorch is an open-source machine learning framework that allows you to write your own neural networks and optimize them efficiently. It was primarily developed by Facebook's AI Research Lab (FAIR). It is based on the Torch library, which is a scientific computing framework with wide support for machine learning algorithms. PyTorch is a Python package that provides two high-level features:\n",
    "* A replacement for NumPy to use the power of GPUs and other accelerators.\n",
    "* An automatic differentiation library that is useful to implement neural networks."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why use PyTorch?\n",
    "1. PyTorch is Pythonic: Ease of using Popular Python packages like NumPy, SciPy, etc to extend the functionality of PyTorch.\n",
    "2. Easy to Learn: Due to its easy and intuitive syntax.\n",
    "3. Strong Community Support: PyTorch has a strong community of developers and researchers who have contributed to the framework. When in doubt, you may most likely see a solution to your problem on https://discuss.pytorch.org/.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch Governence: https://pytorch.org/docs/stable/community/governance.html"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensors"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensors are the basic building blocks of modern deep learning frameworks. A tensor is a generalization of vectors and matrices and is easily understood as a multidimensional array. A tensor can be a number, a vector, a matrix, or an n-dimensional array, or in general terms a container storing numerical values.\n",
    "\n",
    "Recommended viewing to understand tensors: What's a Tensor? - Dan Fleisch (https://www.youtube.com/watch?v=f5liqUk0ZTw)\n",
    "\n",
    "##### Types of Tensor:\n",
    "* Scalar / 0-D tensor: A container with a single value. (Ex: 23, 5, 2, etc)\n",
    "* Vector / 1-D tensor: A container with multiple values. (Ex: [1, 2, 3, 4, 5])\n",
    "* Matrix / 2-D tensor: A container with multiple values arranged in rows and columns. (Ex: [[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "* In general, 2-D tensors are called matrices (Matrix, if singular), and anything above 2-D are just called tensors.\n",
    "\n",
    "##### Rank of a Tensor:\n",
    "\n",
    "A tensor of rank 1 is a vector, which is a one-dimensional array, \n",
    "```python\n",
    "[a,b]\n",
    "```\n",
    "A tensor of rank 2 is a vector of vectors, or a matrix, or a two-dimensional array,\n",
    "```python\n",
    "[[a,b],[c,d]]\n",
    "```\n",
    "A tensor of rank 3 is a vector of vectors of vectors, so something with three nestings,\n",
    "```python\n",
    "[[[a,b],[c,d]],[[e,f],[g,h]]]\n",
    "```\n",
    "and so on...\n",
    "\n",
    "Broadly, a tensor can be understood to be a vector of vectors of vectors of vectors... and so on. The rank is the number of nestings of \"of vectors\". \n",
    "\n",
    "Or in other words, Rank can be described as the number of information needed to specify a particular element of a tensor. For example, to specify an element in a 2-D tensor, we need 2 pieces of information, the row and the column. So, the rank of a 2-D tensor is 2.\n",
    "\n",
    "#### Pitfall:\n",
    "##### Are Matrices and 2-D tensors the same?    \n",
    "\n",
    "A 2-D tensor can be represented by a matrix, but there is more to a tensor than just its arrangement of numerical values. A tensor is a geometric object, whose components obey certain transformation laws. A matrix is just a collection of numbers in a rectangular array, there's no inherent rule that they have to obey any transformation rules.\n",
    "\n",
    "Video is a series of images correlated over time. We can use tensors to represent that correlation better and more intuitively than trying to convert it down to two-dimensional matrices. A third-rank tensor can encode all the aspects of each image (height, width, and color), while a rank-4 tensor could also hold information about time or order for the images."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2., 3.], requires_grad=True)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.tensor([1, 2, 3],\n",
    "                 dtype=torch.float32,\n",
    "                 device='cpu',\n",
    "                 requires_grad=True)\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of scalar: torch.Size([]), vector: torch.Size([3]), matrix: torch.Size([2, 3])\n",
      "Number of dimensions of scalar: 0, vector: 1, matrix: 2\n"
     ]
    }
   ],
   "source": [
    "# scalar\n",
    "scalar = torch.tensor(10)\n",
    "\n",
    "# vector\n",
    "vector = torch.tensor([10, 20, 30])\n",
    "\n",
    "# matrix\n",
    "matrix = torch.tensor([[10, 20, 30], [40, 50, 60]])\n",
    "\n",
    "# To check the shape of a tensor, use the .shape property\n",
    "print(f'Shape of scalar: {scalar.shape}, vector: {vector.shape}, matrix: {matrix.shape}')\n",
    "\n",
    "# To check the number of dimensions of a tensor, use the .ndim property\n",
    "print(f'Number of dimensions of scalar: {scalar.ndim}, vector: {vector.ndim}, matrix: {matrix.ndim}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many other methods for creating tensors, a few of which are listed below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[5., 5., 5.],\n",
      "        [5., 5., 5.]])\n",
      "tensor([[0.2566, 0.7936, 0.9408],\n",
      "        [0.1332, 0.9346, 0.5936]])\n",
      "tensor([[ 2.2082, -0.6380,  0.4617],\n",
      "        [ 0.2674,  0.5349,  0.8094]])\n",
      "tensor([[9, 3, 1],\n",
      "        [9, 7, 9]])\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "t = torch.zeros(2, 3)                # creates a tensor with 0s, by passing the shape as arguments\n",
    "t_zeros = torch.zeros_like(t)        # zeros_like returns a new tensor, with the same shape as the input tensor, but filled with 0s\n",
    "t_ones = torch.ones(2, 3)            # creates a tensor with 1s\n",
    "t_fives = torch.empty(2, 3).fill_(5) # creates a non-initialized tensor and fills it with 5\n",
    "t_random = torch.rand(2, 3)          # creates a uniform random tensor\n",
    "t_normal = torch.randn(2, 3)         # creates a normal random tensor\n",
    "t_randint = torch.randint(low=0, high=10, size=(2, 3)) # creates a random tensor with integers between low and high (exclusive)\n",
    "\n",
    "np_array = np.zeros((2, 3))\n",
    "t_from_np = torch.from_numpy(np_array) # creates a tensor from a numpy array\n",
    "\n",
    "print(t_zeros)\n",
    "print(t_ones)\n",
    "print(t_fives)\n",
    "print(t_random)\n",
    "print(t_normal)\n",
    "print(t_randint)\n",
    "print(t_from_np)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the tensor dimensions can be direct arguments, or a collection like a\n",
    "tuple or list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9516, 0.0753, 0.8860],\n",
      "        [0.5832, 0.3376, 0.8090]])\n",
      "tensor([[0.5779, 0.9040, 0.5547],\n",
      "        [0.3423, 0.6343, 0.3644]])\n",
      "tensor([[0.7104, 0.9464, 0.7890],\n",
      "        [0.2814, 0.7886, 0.5895]])\n"
     ]
    }
   ],
   "source": [
    "t1 = torch.rand(2, 3)\n",
    "print(t1)\n",
    "\n",
    "t2 = torch.rand((2, 3))\n",
    "print(t2)\n",
    "\n",
    "t3 = torch.rand([2, 3])\n",
    "print(t3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The underscore in `t_fives = torch.empty(2, 3).fill_(5)` signifies that the operation is performed in-place. For example, `t_fives.add_(5)` will add 5 to each element of the tensor `t_fives` in-place. Other examples of in-place operations are `t_fives.mul_(5)`, `t_fives.div_(5)`, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[10., 10., 10.],\n",
      "        [10., 10., 10.]])\n",
      "tensor([[10., 10., 10.],\n",
      "        [10., 10., 10.]])\n"
     ]
    }
   ],
   "source": [
    "t_fives_add = t_fives + 5\n",
    "print(t_fives_add)\n",
    "t_fives.add_(5) # in-place addition\n",
    "print(t_fives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.0\n",
      "<class 'float'>\n"
     ]
    }
   ],
   "source": [
    "# Accesing the scalar value of a tensor\n",
    "print(t_fives[0, 1].item()) # item() returns the value of a tensor as a standard Python number\n",
    "print(type(t_fives[0, 1].item())) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operations on Tensors"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshaping Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7539, 0.1952, 0.0050],\n",
      "        [0.3068, 0.1165, 0.9103]])\n",
      "tensor([[0.7539, 0.1952],\n",
      "        [0.0050, 0.3068],\n",
      "        [0.1165, 0.9103]])\n",
      "tensor([0.7539, 0.1952, 0.0050, 0.3068, 0.1165, 0.9103])\n",
      "tensor([[0.7539, 0.1952, 0.0050, 0.3068, 0.1165, 0.9103]])\n",
      "tensor([[0.7539],\n",
      "        [0.1952],\n",
      "        [0.0050],\n",
      "        [0.3068],\n",
      "        [0.1165],\n",
      "        [0.9103]])\n",
      "tensor([[0.7539, 0.1952],\n",
      "        [0.0050, 0.3068],\n",
      "        [0.1165, 0.9103]])\n",
      "tensor([[0.7539, 0.1952],\n",
      "        [0.0050, 0.3068],\n",
      "        [0.1165, 0.9103]])\n"
     ]
    }
   ],
   "source": [
    "# Reshape\n",
    "t = torch.rand(2, 3)\n",
    "print(t)\n",
    "print(t.reshape(3, 2))\n",
    "print(t.reshape(6))\n",
    "print(t.reshape(1, 6))\n",
    "print(t.reshape(6, 1))\n",
    "print(t.reshape(3, -1)) # -1 means \"infer the correct value from the shape of the tensor\"\n",
    "print(t.view(3, -1)) # view is an alternative to reshape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`view` and `reshape` gave the same result. What's the difference? \n",
    "\n",
    "`view()` will try to change the shape of the tensor while keeping the underlying data allocation the same, thus data will be shared between the two tensors. `reshape()` will create a new underlying memory allocation if necessary.\n",
    "\n",
    "When you use `reshape()` instead of view, the matrix is made contiguous if that is necessary, but otherwise the same data is used. This is nice if you don't care much about memory use. If you want to be sure that you're not accidentally copying a large matrix, `view()` is the way to go.\n",
    "\n",
    "Additional Reading: https://stackoverflow.com/questions/49643225/whats-the-difference-between-reshape-and-view-in-pytorch, https://discuss.pytorch.org/t/whats-the-difference-between-torch-reshape-vs-torch-view/159172/3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pytorch Data Types"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Available data types include:\n",
    "\n",
    "```python\n",
    "torch.bool\n",
    "torch.int8\n",
    "torch.uint8\n",
    "torch.int16\n",
    "torch.int32\n",
    "torch.int64\n",
    "torch.half = torch.float16\n",
    "torch.float = torch.float32 (default datatype)\n",
    "torch.double = torch.float64\n",
    "torch.bfloat16\n",
    "```\n",
    "\n",
    "When not specified, the default is `torch.float32`, which differs from plain \n",
    "python where the default floating point datatype is `float64`.\n",
    "\n",
    "Ref: https://pytorch.org/tutorials/beginner/introyt/tensors_deeper_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 1, 1],\n",
      "        [1, 1, 1]], dtype=torch.int16)\n",
      "tensor([[3.0425, 5.3009, 2.6087],\n",
      "        [5.0371, 4.6682, 4.5133]], dtype=torch.float64)\n",
      "tensor([[3, 5, 2],\n",
      "        [5, 4, 4]], dtype=torch.int32)\n",
      "tensor([[-0.5687,  1.2580, -1.5890],\n",
      "        [-1.1208,  0.8423,  0.1744]])\n",
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones((2, 3), dtype=torch.int16) # creates a tensor with 1s of type int16\n",
    "print(a)\n",
    "\n",
    "b = torch.rand((2, 3), dtype=torch.float64) * 20 # creates a random tensor of type float64 and multiplies it by 20\n",
    "print(b)\n",
    "\n",
    "c = b.to(torch.int32) # converts the tensor of type float64 to type int32\n",
    "print(c)\n",
    "\n",
    "d = torch.randn([2, 3]) # creates a random tensor of type float32\n",
    "print(d)\n",
    "print(d.dtype)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also note that when the tensor is the default datatype, the `dtype` is not printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.3383,  1.6992,  0.0109, -0.3387, -1.3407, -0.5854],\n",
      "        [ 0.5362,  0.5246, -1.4692,  1.4332,  0.7440, -0.4816],\n",
      "        [-1.0495,  0.6039, -0.3165,  0.5886, -0.8905,  0.4098],\n",
      "        [-1.4570, -0.1023, -0.5992,  0.4771, -0.1693,  0.2332],\n",
      "        [ 4.0356,  1.2795, -0.0127,  0.2408,  0.1325,  0.7642]]) torch.float32\n",
      "tensor([[ 0.3383,  1.6992,  0.0109, -0.3387, -1.3407, -0.5854],\n",
      "        [ 0.5362,  0.5246, -1.4692,  1.4332,  0.7440, -0.4816],\n",
      "        [-1.0495,  0.6039, -0.3165,  0.5886, -0.8905,  0.4098],\n",
      "        [-1.4570, -0.1023, -0.5992,  0.4771, -0.1693,  0.2332],\n",
      "        [ 4.0356,  1.2795, -0.0127,  0.2408,  0.1325,  0.7642]],\n",
      "       dtype=torch.float64)\n",
      "tensor([[  0,   1,   0,   0, 255,   0],\n",
      "        [  0,   0, 255,   1,   0,   0],\n",
      "        [255,   0,   0,   0,   0,   0],\n",
      "        [255,   0,   0,   0,   0,   0],\n",
      "        [  4,   1,   0,   0,   0,   0]], dtype=torch.uint8)\n"
     ]
    }
   ],
   "source": [
    "t = torch.randn(5, 6)\n",
    "print(t, t.dtype)\n",
    "t = t.double()  # converts to 64-bit float\n",
    "print(t)\n",
    "t = t.byte()    # converts to unsigned 8-bit integer\n",
    "print(t)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ``torch.float16`` vs ``torch.float32``\n",
    "\n",
    "- Reduced Memory Usage:   \n",
    "    - ``torch.float16`` uses half as much memory per element compared to ``torch.float32``. This can be significant when dealing with large neural networks or datasets, especially on GPUs with limited memory.    \n",
    "    - It allows you to store and process larger models or batches of data.  \n",
    "    \n",
    "- Faster Computation and Reduced Bandwidth (While transferring from CPU to GPU, or across network connections)\n",
    "\n",
    "- Trade-offs: \n",
    "    - Reduced Precision - may lead to rounding errors in calculations.\n",
    "    - Not all deep learning models and operations are compatible with ``torch.float16``. Some operations may require ``torch.float32`` or higher precision to maintain accuracy.\n",
    "    - Training with ``torch.float16`` can be more challenging as it can lead to convergence issues, especially in complex models."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mathematical Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a+b = tensor([[2., 2., 2.],\n",
      "        [2., 2., 2.]])\n",
      "\n",
      "c = tensor([[2., 2., 2.],\n",
      "        [2., 2., 2.]])\n",
      "\n",
      "d = tensor([[2., 2., 2.],\n",
      "        [2., 2., 2.]])\n",
      "\n",
      "fours = tensor([[4., 4., 4.],\n",
      "        [4., 4., 4.]])\n",
      "\n",
      "powers2 = tensor([[2., 4., 8.],\n",
      "        [2., 4., 8.]])\n",
      "\n",
      "c/powers2 = tensor([[1.0000, 0.5000, 0.2500],\n",
      "        [1.0000, 0.5000, 0.2500]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(2, 3)\n",
    "b = torch.ones(2, 3)\n",
    "print(f'a+b = {a+b}\\n')\n",
    "\n",
    "c = torch.ones(2, 3) * 2\n",
    "print(f'c = {c}\\n')\n",
    "\n",
    "d = torch.ones(2, 3) + 1 # broadcasting\n",
    "print(f'd = {d}\\n')\n",
    "\n",
    "fours = c ** 2 # element-wise exponentiation\n",
    "print(f'fours = {fours}\\n')\n",
    "\n",
    "powers2 = c ** torch.tensor([1, 2, 3]) # element-wise exponentiation with broadcasting\n",
    "print(f'powers2 = {powers2}\\n')\n",
    "\n",
    "print(f'c/powers2 = {c/powers2}\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Matrix Multiplication: \n",
    "\n",
    "Things to keep in mind:\n",
    "* Inner dimensions must match (Ex: 2x3 and 3x4)\n",
    "* The resulting matrix has the shape of the outer dimensions (Ex: 2x4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4328, 0.2956, 0.2581, 0.1628],\n",
      "        [0.6889, 0.5091, 0.3646, 0.1278]]) torch.Size([2, 4])\n"
     ]
    }
   ],
   "source": [
    "# Matrix multiplication\n",
    "\n",
    "A = torch.rand(2, 3)\n",
    "B = torch.rand(3, 4)\n",
    "C = torch.mm(A, B) # matrix multiplication\n",
    "print(C, C.shape)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are other ways of performing matrix multiplication in PyTorch, such as torch.matmul(), torch.bmm(), and the @ operator. Read more at: https://www.geeksforgeeks.org/python-matrix-multiplication-using-pytorch/"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexing and Slicing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general PyTorch tensors behave similarly to Numpy arrays. They are zero indexed and support slicing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.8712, 0.1330, 0.4137],\n",
      "        [0.6044, 0.7581, 0.9037]])\n",
      "tensor(0.8712)\n",
      "tensor([0.8712, 0.1330, 0.4137])\n",
      "tensor([0.8712, 0.6044])\n",
      "tensor([[0.8712, 0.1330],\n",
      "        [0.6044, 0.7581]])\n"
     ]
    }
   ],
   "source": [
    "t = torch.rand(2, 3)\n",
    "print(t)\n",
    "print(t[0, 0]) # access a single element\n",
    "print(t[0, :]) # access a row\n",
    "print(t[:, 0]) # access a column\n",
    "print(t[0:2, 0:2]) # access a sub-matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.1330, 0.4137],\n",
      "        [0.6044, 0.7581, 0.9037]])\n",
      "tensor([[2.0000, 2.0000, 2.0000],\n",
      "        [0.6044, 0.7581, 0.9037]])\n",
      "tensor([[3.0000, 2.0000, 2.0000],\n",
      "        [3.0000, 0.7581, 0.9037]])\n",
      "tensor([[4.0000, 4.0000, 2.0000],\n",
      "        [4.0000, 4.0000, 0.9037]])\n"
     ]
    }
   ],
   "source": [
    "t[0, 0] = 1 # modify a single element\n",
    "print(t)\n",
    "t[0, :] = 2 # modify a row with implicit broadcasting\n",
    "print(t)\n",
    "t[:, 0] = 3 # modify a column with implicit broadcasting\n",
    "print(t)\n",
    "t[0:2, 0:2] = 4 # modify a sub-matrix with implicit broadcasting\n",
    "print(t)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Commonly used tensor methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before permute: torch.Size([2, 3, 4])\n",
      "After permute: torch.Size([3, 2, 4])\n",
      "After flatten on 2nd dim: torch.Size([3, 8])\n",
      "After flattening the resultant tensor: torch.Size([24])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2],\n",
       "        [0, 1, 2]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.randn(2,3)\n",
    "t.max()                 # returns the maximum value in a tensor\n",
    "t.argmax()              # returns the index of the maximum value in a tensor.\n",
    "t.sum(dim=0)            # sum across rows\n",
    "t.sum(dim=1)            # sum across columns\n",
    "t.t()                   # transpose\n",
    "t.numel()               # number of elements in tensor\n",
    "t.nonzero()             # indices of non-zero elements\n",
    "t.squeeze()             # removes size 1 dimensions\n",
    "t.unsqueeze(0)          # inserts a dimension\n",
    "\n",
    "t = torch.rand(2, 3, 4)\n",
    "print('Before permute:', t.shape)\n",
    "t = t.permute(1, 0, 2)         # permutes dimensions: 1st dim becomes 2nd, 2nd becomes 1st, 3rd remains 3rd\n",
    "print('After permute:', t.shape)\n",
    "t = t.flatten(start_dim=1)     # flattens a tensor from the 2nd dimension\n",
    "print('After flatten on 2nd dim:', t.shape)\n",
    "t = t.flatten()                # flattens a tensor from the 1st dimension by default\n",
    "print('After flattening the resultant tensor:', t.shape)\n",
    "\n",
    "torch.dist(torch.tensor([3.0, 1.0]), torch.tensor([1.0, 2.0]), p=2) # computes the distance between two tensors, Returns the p-norm of (torch.tensor([3.0, 1.0]) - torch.tensor([1.0, 2.0])\n",
    "\n",
    "torch.arange(0, 10)     # tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
    "torch.eye(3, 3)         # creates a 3x3 matrix with 1s in the diagonal (identity in this case)\n",
    "t = torch.arange(0, 3)  # tensor([0, 1, 2])\n",
    "torch.cat((t, t))       # tensor([0, 1, 2, 0, 1, 2])\n",
    "torch.stack((t, t))     # tensor([[0, 1, 2],\n",
    "                        #         [0, 1, 2]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomness"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having a piece of code that behaves randomly and that spews out different results every time you run it is not a good idea. Athough, as programmers, we do not deal with true randomness, we deal with psuedo-randomness. But torch.randn does seem to give out different results everytime we run it, and by definition is random isn't it? \n",
    "\n",
    "In a way, yes. But, the random numbers are not quite truly random. They are pseudo-random numbers, which means that a number generator is used to generate a sequence of numbers that appear to be random, but they are not. The sequence of numbers generated by a pseudo-random number generator is determined by a fixed initial value called the seed. Every time we give it the same seed, it will give us the same sequence of numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[False, False, False],\n",
      "        [False, False, False]])\n"
     ]
    }
   ],
   "source": [
    "# Without setting the seed, the results of the random operations will be different every time\n",
    "\n",
    "random_1 = torch.rand(2, 3)\n",
    "random_2 = torch.rand(2, 3)\n",
    "\n",
    "print(random_1 == random_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[False, False, False],\n",
      "        [False, False, False]])\n",
      "\n",
      "tensor([[True, True, True],\n",
      "        [True, True, True]])\n",
      "tensor([[True, True, True],\n",
      "        [True, True, True]])\n",
      "\n",
      "tensor([[True, True, True],\n",
      "        [True, True, True]])\n"
     ]
    }
   ],
   "source": [
    "# By setting the seed, the results of the random operations will be the same every time\n",
    "\n",
    "torch.manual_seed(42)\n",
    "random_1 = torch.rand(2, 3)\n",
    "random_2 = torch.rand(2, 3)     # the second call to torch.rand(2, 3) will return different values than the first call\n",
    "print(random_1 == random_2)\n",
    "print()\n",
    "\n",
    "torch.manual_seed(42)\n",
    "random_3 = torch.rand(2, 3)\n",
    "random_4 = torch.rand(2, 3)     \n",
    "print(random_1 == random_3)\n",
    "print(random_2 == random_4)\n",
    "print()     \n",
    "\n",
    "torch.manual_seed(42)\n",
    "random_5 = torch.rand(2, 3)\n",
    "torch.manual_seed(42)           # need to reset the seed again, to get the same values for the second call to torch.rand(2, 3)\n",
    "random_6 = torch.rand(2, 3)\n",
    "print(random_5 == random_6)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When training a neural network, we use random initialization of weights, shuffling of data during training, and dropout, among other techniques. All these involve randomness, which can lead to slightly different results each time we run the code unless we set a seed. This can be problematic when trying to replicate someone elseâ€™s results or debugging your own code.\n",
    "\n",
    "Additional Reading: https://pieriantraining.com/how-to-set-the-seed-in-pytorch-for-reproducible-results/"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
