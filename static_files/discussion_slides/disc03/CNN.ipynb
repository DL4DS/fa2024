{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Computer vision libraries in PyTorch\n",
    "\n",
    "| PyTorch module | What does it do? |\n",
    "| ----- | ----- |\n",
    "| [`torchvision`](https://pytorch.org/vision/stable/index.html) | Contains datasets, model architectures and image transformations often used for computer vision problems. |\n",
    "| [`torchvision.datasets`](https://pytorch.org/vision/stable/datasets.html) | Here you'll find many example computer vision datasets for a range of problems from image classification, object detection, image captioning, video classification and more. It also contains [a series of base classes for making custom datasets](https://pytorch.org/vision/stable/datasets.html#base-classes-for-custom-datasets). |\n",
    "| [`torchvision.models`](https://pytorch.org/vision/stable/models.html) | This module contains well-performing and commonly used computer vision model architectures implemented in PyTorch, you can use these with your own problems. | \n",
    "| [`torchvision.transforms`](https://pytorch.org/vision/stable/transforms.html) | Often images need to be transformed (turned into numbers/processed/augmented) before being used with a model, common image transformations are found here. | \n",
    "| [`torch.utils.data.Dataset`](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset) | Base dataset class for PyTorch.  | \n",
    "| [`torch.utils.data.DataLoader`](https://pytorch.org/docs/stable/data.html#module-torch.utils.data) | Creates a Python iterable over a dataset (created with `torch.utils.data.Dataset`). |    \n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Typical CNN Framework"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](https://ujwlkarn.files.wordpress.com/2016/08/screen-shot-2016-08-07-at-4-59-29-pm.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What does a CNN do?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](https://ujwlkarn.files.wordpress.com/2016/07/screen-shot-2016-07-24-at-11-25-13-pm.png?w=254&h=230)\n",
    "![title](https://ujwlkarn.files.wordpress.com/2016/07/screen-shot-2016-07-24-at-11-25-24-pm.png?w=148&h=128)\n",
    "\n",
    "![title](https://ujwlkarn.files.wordpress.com/2016/07/convolution_schematic.gif)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](https://ujwlkarn.files.wordpress.com/2016/08/giphy.gif)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eoVNuUXZy5W2zb0CIK5IhQ.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Practice a CNN will learn the values of these filters on its own, during the training process. Although, we would need to define the parameters of `nn.Conv2d()`:\n",
    "\n",
    "* `in_channels` (int) - Number of channels in the input image.\n",
    "* `out_channels` (int) - Number of channels produced by the convolution. (Number of filters)\n",
    "* `kernel_size` (int or tuple) - Size of the convolving kernel/filter.\n",
    "* `stride` (int or tuple, optional) - How big of a step the convolving kernel takes at a time. Default: 1.\n",
    "* `padding` (int, tuple, str) - Padding added to all four sides of input. We can add extra pixels around the edges of the input image to make sure the filter properly passes over the edges of the image. A feature of zero padding is that it will allow us to control the spatial size of the output volumes. Used when its required to preserve the spatial size of the input volume so the input and output width and height are the same. Default: 0.\n",
    "\n",
    "![example of going through the different parameters of a Conv2d layer](https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/03-conv2d-layer.gif)\n",
    "\n",
    "*Example of what happens when you change the hyperparameters of a `nn.Conv2d()` layer.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "torch.Size([1, 16, 64, 64])\n",
      "\n",
      "Conv2d(3, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "torch.Size([1, 16, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "conv_layer = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
    "tensor = torch.randn(1, 3, 64, 64)\n",
    "output = conv_layer(tensor)\n",
    "print(conv_layer)\n",
    "print(output.size())\n",
    "print()\n",
    "\n",
    "conv_layer_2 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=4, stride=2, padding=1)\n",
    "output_2 = conv_layer_2(tensor)\n",
    "print(conv_layer_2)\n",
    "print(output_2.size())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compute the spatial size of the output volume as a function of the input volume size (W), the receptive field size of the Conv Layer neurons (F), the stride with which they are applied (S), and the amount of zero padding used (P) on the border. You can convince yourself that the correct formula for calculating how many neurons “fit” is given by     \n",
    "    \n",
    "O = (W−F+2P)/S+1.    \n",
    "    \n",
    "For example for a 7x7 input and a 3x3 filter with stride 1 and pad 0 we would get a 5x5 output. With stride 2 we would get a 3x3 output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pooling Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 64, 64])\n",
      "MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "torch.Size([1, 3, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "max_pool_layer = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "output_3 = max_pool_layer(tensor)\n",
    "print(tensor.shape)\n",
    "print(max_pool_layer)\n",
    "print(output_3.size())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](https://ujwlkarn.files.wordpress.com/2016/08/screen-shot-2016-08-10-at-3-38-39-am.png?w=988)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pooling layer downsamples the volume spatially, independently in each depth slice of the input volume. Left: In this example, the input volume of size [224x224x64] is pooled with filter size 2, stride 2 into output volume of size [112x112x64]. Notice that the volume depth is preserved. Right: The most common downsampling operation is max, giving rise to max pooling, here shown with a stride of 2. That is, each max is taken over 4 numbers (little 2x2 square)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](https://ujwlkarn.files.wordpress.com/2016/08/screen-shot-2016-08-07-at-6-11-53-pm.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function of Pooling is to progressively reduce the spatial size of the input representation.\n",
    "* It reduces the amount of parameters and computation in the network, and hence to also control overfitting.\n",
    "* It makes the detection of features invariant to small transformations, distortions and translations. (a small distortion in input will not change the output of Pooling – since we take the maximum / average value in a local neighborhood)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting it all together:\n",
    "\n",
    "\n",
    "\n",
    "![title](https://ujwlkarn.files.wordpress.com/2016/08/screen-shot-2016-08-07-at-9-15-21-pm.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](https://ujwlkarn.files.wordpress.com/2016/08/screen-shot-2016-08-10-at-12-58-30-pm.png?w=484)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Conv Layers - Transformations    \n",
    "Pooling Layers - Downsampling\n",
    "\n",
    "Key Principles of Convolutional Neural Networks:\n",
    "\n",
    "- Sparse Interactions: In traditional neural networks, each output unit interacts with every input unit via matrix multiplication, resulting in dense connections. However, CNNs employ sparse interaction by using smaller kernels compared to the input data. For instance, while processing an image with millions of pixels, kernels can capture meaningful information within a smaller window of tens or hundreds of pixels. This sparse interaction reduces the number of parameters needed, leading to lower memory requirements and enhanced statistical efficiency of the model. It allows CNNs to focus on local patterns, efficiently capturing relevant features while ignoring irrelevant ones.\n",
    "\n",
    "- Parameter Sharing: Traditional neural networks lack parameter sharing, leading to distinct weights applied only once and never revisited. However, in CNNs, the same set of weights is reused across the input, promoting efficient feature extraction and allowing the network to learn patterns that are applicable in various spatial locations.\n",
    "\n",
    "- Equivariant Representations: Parameter sharing in CNN layers results in equivariance to translation. This property implies that if a transformation occurs in the input, the output undergoes a corresponding transformation. For instance, if an image is shifted or translated, the learned features or patterns in the output also shift accordingly. CNNs’ equivariant nature ensures that learned features retain their spatial relationships, making them robust and effective in recognizing patterns regardless of their position within the input data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transposed Convolution\n",
    "\n",
    "Where would you find them? - In the decoder part of an autoencoder, in the generator part of a GAN, in the upsampling part of a U-Net, etc.\n",
    "\n",
    "Purpose? - To increase the size or spatial resolution of the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([1, 16, 4, 4])\n",
      "Output shape: torch.Size([1, 3, 8, 8])\n"
     ]
    }
   ],
   "source": [
    "conv_transpose = nn.ConvTranspose2d(in_channels=16, out_channels=3, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "input = torch.randn(1, 16, 4, 4)\n",
    "output = conv_transpose(input)\n",
    "\n",
    "print(\"Input shape:\", input.shape)\n",
    "print(\"Output shape:\", output.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Is it the reverse of a Convolution?\n",
    "\n",
    "- Not really, if you think about it, convolution can't be \"reverted\". You won't be able to recover the original values given the output of a convolutional layer. (Example below: Convolution results in a loss of information (around the edges of the image).)\n",
    "\n",
    "- Although we cant revert the information lost, we can reverse the size reduction that happens in a convolutional layer. - Which is what transposed convolution does.\n",
    "\n",
    "\n",
    "![title](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SEvLW8SoeUqTkNZgZNFCNQ.png)\n",
    "\n",
    "Example to show that convolution results in a loss of information (around the edges of the image)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](https://miro.medium.com/v2/resize:fit:1000/format:webp/1*SpxCUPzNfb9C8TiAcrRr5A.gif)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dilated Convolution"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is a technique that expands the kernel (input) by inserting holes between its consecutive elements. In simpler terms, it is the same as convolution but it involves pixel skipping, so as to cover a larger area of the input. \n",
    "\n",
    "- Enables the network to have a larger receptive field without increasing the number of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([1, 1, 8, 8])\n",
      "Output shape: torch.Size([1, 1, 6, 6])\n"
     ]
    }
   ],
   "source": [
    "dilated_conv = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=3, stride=1, padding=1, dilation=2)\n",
    "# dilation = 2 means there is one pixel of space between the kernel elements\n",
    "\n",
    "input = torch.randn(1, 1, 8, 8)\n",
    "\n",
    "# Apply the dilated convolutional layer to the input\n",
    "output = dilated_conv(input)\n",
    "\n",
    "print(\"Input shape:\", input.shape)\n",
    "print(\"Output shape:\", output.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](https://github.com/vdumoulin/conv_arithmetic/raw/master/gif/no_padding_no_strides.gif)    \n",
    "Convolution   \n",
    "\n",
    "![title](https://github.com/vdumoulin/conv_arithmetic/raw/master/gif/no_padding_no_strides_transposed.gif)    \n",
    "Transposed Convolution    \n",
    "\n",
    "![title](https://github.com/vdumoulin/conv_arithmetic/raw/master/gif/dilation.gif)    \n",
    "Dilated Convolution\n",
    "\n",
    "Blue maps are inputs, cyan maps are outputs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning / Fine-Tuning with PyTorch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "What is transfer learning?\n",
    "\n",
    "**Transfer learning** allows us to take the patterns (also called weights) another model has learned from another problem and use them for our own problem.For example, we can take the patterns a computer vision model has learned from datasets such as [ImageNet](https://www.image-net.org/) (millions of images of different objects) and use them to a Custom Dataset. Or we could take the patterns from a [language model](https://developers.google.com/machine-learning/glossary#masked-language-model) (a model that's been through large amounts of text to learn a representation of language) and use them as the basis of a model to classify different text samples. The premise remains: find a well-performing existing model and apply it to your own problem.<img src=\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/06-transfer-learning-example-overview.png\" alt=\"transfer learning overview on different problems\" width=900/>*Example of transfer learning being applied to computer vision and natural language processing (NLP). In the case of computer vision, a computer vision model might learn patterns on millions of images in ImageNet and then use those patterns to infer on another problem. And for NLP, a language model may learn the structure of language by reading all of Wikipedia (and perhaps more) and then apply that knowledge to a different problem.*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why use transfer learning?\n",
    "\n",
    "There are two main benefits to using transfer learning:\n",
    "\n",
    "1. Can leverage an existing model (usually a neural network architecture) proven to work on problems similar to our own.\n",
    "2. Can leverage a working model which has **already learned** patterns on similar data to our own. This often results in achieving **great results with less custom data**.\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Where to find pretrained models\n",
    "\n",
    "| **Location** | **What's there?** | **Link(s)** | \n",
    "| ----- | ----- | ----- |\n",
    "| **PyTorch domain libraries** | Each of the PyTorch domain libraries (`torchvision`, `torchtext`) come with pretrained models of some form. The models there work right within PyTorch. | [`torchvision.models`](https://pytorch.org/vision/stable/models.html), [`torchtext.models`](https://pytorch.org/text/main/models.html), [`torchaudio.models`](https://pytorch.org/audio/stable/models.html), [`torchrec.models`](https://pytorch.org/torchrec/torchrec.models.html) |\n",
    "| **HuggingFace Hub** | A series of pretrained models on many different domains (vision, text, audio and more) from organizations around the world. There's plenty of different datasets too. | https://huggingface.co/models, https://huggingface.co/datasets | \n",
    "| **`timm` (PyTorch Image Models) library** | Almost all of the latest and greatest computer vision models in PyTorch code as well as plenty of other helpful computer vision features. | https://github.com/rwightman/pytorch-image-models|\n",
    "| **Paperswithcode** | A collection of the latest state-of-the-art machine learning papers with code implementations attached. You can also find benchmarks here of model performance on different tasks. | https://paperswithcode.com/ | \n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/06-transfer-learning-where-to-find-pretrained-models.png\" alt=\"different locations to find pretrained neural network models\" width=900/>\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](https://miro.medium.com/v2/resize:fit:1400/format:webp/0*8Z3To8OAwBBIj66p.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/projectnb/ivc-ml/xthomas/.conda/envs/dl4ds_tutor/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original FC layer: Linear(in_features=2048, out_features=1000, bias=True)\n",
      "Feature dimension: 2048\n",
      "Modified FC layer: Sequential(\n",
      "  (0): Linear(in_features=2048, out_features=256, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Dropout(p=0.5, inplace=False)\n",
      "  (3): Linear(in_features=256, out_features=4, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import timm\n",
    "\n",
    "num_classes = 4 # Replace num_classes with the number of classes in your data\n",
    "\n",
    "# Load pre-trained model from timm\n",
    "model = timm.create_model('resnet50', pretrained=True)\n",
    "print(f'Original FC layer: {model.fc}')\n",
    "\n",
    "# Modify the model head for fine-tuning\n",
    "num_features = model.fc.in_features\n",
    "print(f'Feature dimension: {num_features}')\n",
    "\n",
    "# Additional linear layer and dropout layer\n",
    "model.fc = nn.Sequential(\n",
    "    nn.Linear(num_features, 256),  \n",
    "    nn.ReLU(),         \n",
    "    nn.Dropout(0.5),               \n",
    "    nn.Linear(256, num_classes)    \n",
    ")\n",
    "print(f'Modified FC layer: {model.fc}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Freezing Full or Partial network"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Freezing - fixing the weight of specific layer or entire network during fine tuning process. Network freezing allows us to retain the knowledge captured by the pre-trained model while only updating certain layers to adapt to the target task."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- if the pre-trained model has been trained on a large-scale dataset similar to the target task, freezing the entire network can help preserve the learned representations, preventing them from being overwritten. In this case, only the model’s head is modified and trained from scratch.\n",
    "\n",
    "- freezing only a portion of the network. This approach is particularly useful when the pre-trained model has been trained on a dataset that is somewhat similar to the target task, but you believe that fine-tuning some of the more abstract or higher-level features could further improve performance. By doing so, you can adjust the deeper layers of the network to better suit the nuances of your specific task while still leveraging the lower-level feature representations learned from the large-scale dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = timm.create_model('resnet50', pretrained=True)\n",
    "\n",
    "# Freeze all the layers of the pre-trained model\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Modify the model's head for a new task\n",
    "num_classes = 10\n",
    "model.fc = nn.Linear(model.fc.in_features, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze only the convolutional layers of the pre-trained model\n",
    "for param in model.parameters():\n",
    "    if isinstance(param, nn.Conv2d):\n",
    "        param.requires_grad = False\n",
    "\n",
    "# Modify the model's head for a new task\n",
    "num_classes = 10\n",
    "model.fc = nn.Linear(model.fc.in_features, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze specific layers (e.g.,the first two convolutional layers) of the pre-trained model\n",
    "for name, param in model.named_parameters():\n",
    "    if 'conv1' in name or 'layer1' in name:\n",
    "        param.requires_grad = False\n",
    "\n",
    "# Modify the model's head for a new task\n",
    "num_classes = 10\n",
    "model.fc = nn.Linear(model.fc.in_features, num_classes)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way of fine-tuning is setting different learning rates for different parts of the network. For example, the learning rate for the pre-trained layers can be set to a smaller value than the learning rate for the new layers. This is because the pre-trained layers may already contain useful representations, and we don’t want to change them too much. On the other hand, the new layers are randomly initialized and need to be trained more aggressively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "optimizer = torch.optim.Adam([\n",
    "    {'params': model.conv1.parameters(), 'lr': 1e-4},\n",
    "    {'params': model.layer1.parameters(), 'lr': 1e-4},\n",
    "    {'params': model.fc.parameters(), 'lr': 1e-3}\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "# Creating a CNN class\n",
    "class ConvNeuralNet(nn.Module):\n",
    "\t#  Determine what layers and their order in CNN object \n",
    "    def __init__(self, num_classes):\n",
    "        super(ConvNeuralNet, self).__init__()\n",
    "\n",
    "        self.conv_layers = nn.ModuleDict({\n",
    "            'conv1': nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3),\n",
    "            'conv2': nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3),\n",
    "            'max_pool1': nn.MaxPool2d(kernel_size = 2, stride = 2),\n",
    "            'conv3': nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3),\n",
    "            'conv4': nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3),\n",
    "            'max_pool2': nn.MaxPool2d(kernel_size = 2, stride = 2)\n",
    "        })\n",
    "\n",
    "        self.fc_layers = nn.ModuleDict({\n",
    "            'fc1': nn.Linear(1600, 128),\n",
    "            'fc2': nn.Linear(128, num_classes)\n",
    "        })\n",
    "    \n",
    "    # Progresses data across layers    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        print(f\"Input shape: {x.shape}\")\n",
    "        print()\n",
    "        for layer_name, layer in self.conv_layers.items():\n",
    "            x = layer(x)\n",
    "            print(f\"Shape after {layer_name}: {x.shape}\")\n",
    "\n",
    "        print()\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        print(f\"Flattened shape after Conv Layers: {x.shape}\")\n",
    "        print()\n",
    "\n",
    "        for layer_name, layer in self.fc_layers.items():\n",
    "            x = layer(x)\n",
    "            print(f\"Shape after {layer_name}: {x.shape}\")\n",
    "        return x\n",
    "\n",
    "cnn_model = ConvNeuralNet(num_classes=10)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([16, 3, 32, 32])\n",
      "\n",
      "Shape after conv1: torch.Size([16, 32, 30, 30])\n",
      "Shape after conv2: torch.Size([16, 32, 28, 28])\n",
      "Shape after max_pool1: torch.Size([16, 32, 14, 14])\n",
      "Shape after conv3: torch.Size([16, 64, 12, 12])\n",
      "Shape after conv4: torch.Size([16, 64, 10, 10])\n",
      "Shape after max_pool2: torch.Size([16, 64, 5, 5])\n",
      "\n",
      "Flattened shape after Conv Layers: torch.Size([16, 1600])\n",
      "\n",
      "Shape after fc1: torch.Size([16, 128])\n",
      "Shape after fc2: torch.Size([16, 10])\n"
     ]
    }
   ],
   "source": [
    "random_data = torch.randn(16, 3, 32, 32)\n",
    "output = cnn_model(random_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References at References.md\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds598",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
