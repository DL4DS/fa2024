{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recurrent Neural Networks (Recap)\n",
    "\n",
    "- Why the need for RNNs?\n",
    "    - Feedforward neural networks are not designed to handle sequential/temporal data. Not all problems can be converted into one with fixed- length inputs and outputs!\n",
    "    - Feedforward neural networks (and Convolutional neural networks), can only take in a fixed sized vector as input and produce a fixed sized vector as output, using a fixed size of computational steps (layers). Recurrent Nets allows us to operate over sequences of vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![RNN](https://cdn.jsdelivr.net/gh/AuthurWhywait/images/20211202145041.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The networks preidctions are not only a function of the input at a particular time step, but also the past memory of cell states denoted by the hidden state (h). --> output is now a function of both the current input and the past memory at a previous time step. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ h_t = f(W(x_t, h_{t-1})) $$\n",
    "\n",
    "1. $h_t$ is a cell state;\n",
    "2. $f_W$ is a function with weights $W$;\n",
    "3. $x_t$ is the input;\n",
    "4. $h_{t−1}$ is the old state.\n",
    "\n",
    "NOTE: the same function and set of parameters are used at every time step in each iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![RNN](https://cdn.jsdelivr.net/gh/AuthurWhywait/images/20211202151931.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goal:\n",
    "\n",
    "- At each step the RNN needs to combine the information from the current input with the knowledge it has accumulated so far.\n",
    "- Then based on a transformation of the current input and the previous hidden state, it produces an output and a new hidden state.\n",
    "- The hidden state represents the RNN's memory of the sequence it has seen so far.\n",
    "- This hidden state will influence the next time step, allowing the network to capture sequential information."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](http://karpathy.github.io/assets/rnn/diags.jpeg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1) Traditional Feed Forward Neural Network    \n",
    "(2) Sequence Output (Image Captioning)    \n",
    "(3) Sequence Input (Sentiment Analysis)    \n",
    "(4) Sequence Input and Sequence Output (Machine Translation)    \n",
    "(5) Synced Sequence Input and Output (Video Classification)    \n",
    "\n",
    "Note: RNNs utilize the same set of weights at each time step, thus the number of parameters do not increase with the size of the input. This is a significant advantage when dealing with long  and variable length sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![RNN](https://cdn.jsdelivr.net/gh/AuthurWhywait/images/20211202152611.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using an RNN for a Character-Level Language Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What goes into the model:\n",
    "- X: A sequence of characters (eg. a sentence, a document, a word)\n",
    "- Example: \"Hi, Hell\"\n",
    "\n",
    "#### What will the model do:\n",
    "- Learn to predict the next character in the sequence. That is, we get the model to give the probability distribution of the next character in the sequence given the sequence of previous characters.\n",
    "- Example: Given the sequence \"Hi, Hell\", the model should predict the next character as \"o\" with high probability.\n",
    "- The model continues to predict the next character given the sequence of previous characters and the predicted characters.\n",
    "\n",
    "Another Example:\n",
    "\n",
    "Suppose we only had a vocabulary of four possible letters “helo”, and wanted to train an RNN on the training sequence “hello”. This training sequence is in fact a source of 4 separate training examples: \n",
    "1. The probability of “e” should be likely given the context of “h”, \n",
    "2. “l” should be likely in the context of “he”, \n",
    "3. “l” should also be likely given the context of “hel”, and finally \n",
    "4. “o” should be likely given the context of “hell”.\n",
    "\n",
    "Image bellow: RNN is passed the sequence \"hell\" and it should predict the next character as \"o\" with high probability. (Which it does, highest score is at index 3 for \"o\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](http://karpathy.github.io/assets/rnn/charseq.jpeg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How is training done?\n",
    "\n",
    "- We train the model to minimize the cross-entropy loss between the true distribution of the next character and the predicted distribution.\n",
    "- Example: When the model gets the first character 'h' (at the first time step), it should predict the next character 'e' with high probability. Thereby, we tweak the model's parameters such that it gets better at predicting the next character ('e') given the first character ('h'). And so on for the rest of the sequence."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note:\n",
    "\n",
    "- Notice also that the first time the character “l” is input, the target is “l”, but the second time the target is “o”. The RNN therefore cannot rely on the input alone and must use its recurrent connection to keep track of the context to achieve this task."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How is testing/eval done?\n",
    "\n",
    "- At test time, we feed a character into the RNN and get a distribution over what characters are likely to come next. We sample from this distribution, and feed it right back in to get the next letter. Repeat this process and you’re sampling text!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problems with RNNs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](https://cdn.jsdelivr.net/gh/AuthurWhywait/images/20211202155526.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient flows backwards through all the time steps (from the loss at the end to the first time step). This is called backpropagation through time (BPTT). This means the gradient computation involves recurrent multiplication of $W$. This can lead to the following problems:\n",
    "\n",
    "- Vanishing Gradients: If the recurrent weight matrix has small singular values, the gradients will vanish as we backpropagate to earlier time steps. This means that the model is not able to capture long range dependencies in the data.\n",
    "\n",
    "- Exploding Gradients: If the recurrent weight matrix has large singular values, the gradients will explode as we backpropagate to earlier time steps. This means that the model will not be able to learn effectively. Could be solved by clipping the gradients. (i.e. if gradient is larger than a threshold, clip it to the threshold)\n",
    "\n",
    "Why are vanishing gradients a problem? \n",
    "\n",
    "- When you keep multiplying a small number by another small number, the number is going to keep shrinking and shrinking and eventually going to vanish.\n",
    "- So gradients near the beginning of the sequence are going to be very small and the model is not going to learn effectively, and loosing out on long range dependencies.\n",
    "\n",
    "### Long Short Term Memory (LSTM) Networks\n",
    "- aims to solve the vanishing gradient problem\n",
    "\n",
    "- 3 types of gates:\n",
    "    - Forget gate: Decides what information to throw away from the cell state.\n",
    "    - Input gate: Decides what new information to store in the cell state.\n",
    "    - Output gate: Decides what to output based on input and the memory of the cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In standard RNN, the repeating modules contain a simple computation node:\n",
    "\n",
    "![RNN](https://cdn.jsdelivr.net/gh/AuthurWhywait/images/20211202164941.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM modules contain computational blocks that control information flow.\n",
    "\n",
    "![RNN](https://cdn.jsdelivr.net/gh/AuthurWhywait/images/20211202165435.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Information is added or removed through structures called gates. Gates optionally let information through, for example via a sigmoid neural net layer and pointwise multiplication."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do LSTMs work?\n",
    "\n",
    "1. Forget. LSTMs forget irrelevant parts of the previous state.\n",
    "2. Store. LSTMs store relevant new information into the cell state.\n",
    "3. Update. LSTMs selectively update cell state values.\n",
    "4. Output. The output gate controls what information is sent to the next time step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![RNN](https://cdn.jsdelivr.net/gh/AuthurWhywait/images/20211202170304.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Forget gate: Decides what information to throw away from the cell state.\n",
    "\n",
    "$$ f_t = \\sigma(W_f \\cdot [h_{t-1}, x_t] + b_f) $$\n",
    "\n",
    "* This decision is made by a sigmoid layer called the “forget gate layer.” The forget gate takes the current input $x_t$ and the previous hidden state $h_{t-1}$, and applies a sigmoid function to determine which parts of the cell state to retain and which parts to forget. \n",
    "\n",
    "* A value of 0 means \"completely forget\" that element of the cell state.\n",
    "\n",
    "* A value of 1 means \"completely retain\" that element.\n",
    "\n",
    "* Values between 0 and 1 represent partial retention, meaning the element is scaled down.\n",
    "\n",
    "* How its used:\n",
    "\n",
    "    * The cell vector contains various values that represent the memory from the previous time step.\n",
    "    * The forget gate generates a vector of values (each between 0 and 1) that is the same size as the cell state.\n",
    "    * Each element of the cell state is multiplied by the corresponding value from the forget gate:\n",
    "        * If the forget gate value is close to 1, that part of the cell state remains mostly unchanged.\n",
    "        * If the forget gate value is close to 0, that part of the cell state is nearly \"forgotten.\"\n",
    "\n",
    "    * In Essence, helping the LSTM control how much past information is carried forward or discarded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![RNN](https://cdn.jsdelivr.net/gh/AuthurWhywait/images/20211202170357.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Input gate: Decides what new information to store in the cell state.\n",
    "\n",
    "- Sigmoid Activation for Filtering New Information\n",
    "    - Sigmoid activation function on the input and the previous hidden state (concatenated). This acts as a filter, deciding how much of the new information to add to the cell state.    \n",
    "    - Used to control the flow of new information into the cell state. Deciding at each step whether it should update the memory (cell state) with new content or stick to what it already knows.\n",
    "\n",
    "- Candidate Cell State (Tanh Activation)\n",
    "    - To generate what new information should be considered\n",
    "    - The Tanh activation function creates a candidate cell state, which represents the potential new information to be added to the cell state.\n",
    "\n",
    "- Multiplying Sigmoid and Tanh Outputs\n",
    "    - The LSTM needs to control how much of the candidate cell state to add to the current cell state.\n",
    "    - Sigmoid output (values between 0 and 1) gets multiplied element-wise with the candidate cell state. Thus scales the new information, allowing the LSTM to decie how much to add.\n",
    "    - If sigmoid output is closer `0`, the LSTM will add less of the candidate cell state, and if closer to `1`, it will add more.\n",
    "    - Ensures that only the relevant parts of the new information is introduced into long-term memory.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![RNN](https://cdn.jsdelivr.net/gh/AuthurWhywait/images/20211202170441.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Update\n",
    "\n",
    "* Combine the old cell state with the new candidate values to update the cell state.\n",
    "\n",
    "* Element-Wise Update of the Cell State\n",
    "    - The LSTM's cell state is like a memory bank. To keep useful long-term info while adding new relevant updates, it must selectively combine new information with the existing memory. (Balance between retaining old information and adding new information)\n",
    "    - The existting cell state is element-eise updated:\n",
    "        - Some parts of the current cell state are kept (based on the forget gate’s decision)\n",
    "        - New information is added (from the scaled candidate state)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![RNN](https://cdn.jsdelivr.net/gh/AuthurWhywait/images/20211202170734.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Output gate: Decides what to output based on input and the memory of the cell.\n",
    "\n",
    "- The output gate controls how much information from the cell state should be transferred to the hidden state (which is the actual output at each time step)\n",
    "- The LSTM needs to decide which parts of the updated cell state are relevant to pass along for further processing, and which parts can be kept internal to the cell state.\n",
    "- This step ensures that the output is a filtered version of the cell state, containing only the relevant information.\n",
    "\n",
    "- Sigmoid Activation\n",
    "    - Uses the current input and the previous hidden state to decide how much of the cell state should be used to generate the hidden state.\n",
    "\n",
    "- Tanh Activation on the Cell State\n",
    "    - The cell state (which holds the memory of the LSTM) is passed through a Tanh activation function to compress the values to be between -1 and 1.\n",
    "\n",
    "- Elemnt-wise Multiplication\n",
    "    - The LSTM needs to control how much of the scaled cell state (from the tanh function) should be included in the final hidden state at this time step.\n",
    "    - controls how much of each part of the cell state becomes part of the hidden state.\n",
    "    \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*goJVQs-p9kgLODFNyhl9zA.gif)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recommended Read: \n",
    "\n",
    "https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21\n",
    "\n",
    "https://colah.github.io/posts/2015-08-Understanding-LSTMs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim, dropout_prob):\n",
    "        super(RNNModel, self).__init__()\n",
    "\n",
    "        # Defining the number of layers and the nodes in each layer\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.layer_dim = layer_dim\n",
    "\n",
    "        # RNN layers\n",
    "        self.rnn = nn.RNN(\n",
    "            input_dim, hidden_dim, layer_dim, batch_first=True, dropout=dropout_prob\n",
    "        )\n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.rnn(x)\n",
    "\n",
    "        # Reshaping the outputs in the shape of (batch_size, seq_length, hidden_size)\n",
    "        # so that it can fit into the fully connected layer\n",
    "        out = out[:, -1, :] # Get the last time step output\n",
    "\n",
    "        # Convert the final state to our desired output shape (batch_size, output_dim)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim, dropout_prob):\n",
    "        super(LSTMModel, self).__init__()\n",
    "\n",
    "        # Defining the number of layers and the nodes in each layer\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.layer_dim = layer_dim\n",
    "\n",
    "        # LSTM layers\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_dim, hidden_dim, layer_dim, batch_first=True, dropout=dropout_prob\n",
    "        )\n",
    "\n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "\n",
    "        # Reshaping the outputs in the shape of (batch_size, seq_length, hidden_size)\n",
    "        # so that it can fit into the fully connected layer\n",
    "        out = out[:, -1, :] # Get the last time step output\n",
    "\n",
    "        # Convert the final state to our desired output shape (batch_size, output_dim)\n",
    "        out = self.fc(out)\n",
    "\n",
    "        return out\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import nltk\n",
    "from collections import Counter\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read document\n",
    "with open ('data/tinyshakespeare.txt', 'r') as f:\n",
    "    doc = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SequentialModule(nn.Module):\n",
    "    # initialize module\n",
    "    def __init__(self, n_vocab, seq_size=32, embedding_size=64, lstm_size=64):\n",
    "        super(SequentialModule, self).__init__()\n",
    "        self.seq_size = seq_size\n",
    "        self.lstm_size = lstm_size\n",
    "        self.embedding = nn.Embedding(n_vocab, embedding_size)\n",
    "        self.lstm = nn.LSTM(embedding_size,\n",
    "                            lstm_size,\n",
    "                            batch_first=True)\n",
    "        self.dense = nn.Linear(lstm_size, n_vocab)\n",
    "        \n",
    "    def forward(self, x, prev_state):\n",
    "        embed = self.embedding(x)\n",
    "        output, state = self.lstm(embed, prev_state)\n",
    "        logits = self.dense(output)\n",
    "\n",
    "        return logits, state\n",
    "    \n",
    "    def zero_state(self, batch_size):\n",
    "        return (torch.zeros(1, batch_size, self.lstm_size),torch.zeros(1, batch_size, self.lstm_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "seq_size = 32\n",
    "embedding_size = 64\n",
    "lstm_size = 64\n",
    "gradients_norm = 5\n",
    "# set device parameter\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get list of words from document\n",
    "def doc2words(doc):\n",
    "    lines = doc.split('\\n')\n",
    "    lines = [line.strip(r'\\\"') for line in lines]\n",
    "    words = ' '.join(lines).split()\n",
    "    return words\n",
    "\n",
    "def removepunct(words):\n",
    "    punct = set(string.punctuation)\n",
    "    words = [''.join([char for char in list(word) if char not in punct]) for word in words]\n",
    "    return words\n",
    "\n",
    "# get vocab from word list\n",
    "def getvocab(words):\n",
    "    wordfreq = Counter(words)\n",
    "    sorted_wordfreq = sorted(wordfreq, key=wordfreq.get)\n",
    "    return sorted_wordfreq\n",
    "\n",
    "# get dictionary of int to words and word to int\n",
    "def vocab_map(vocab):\n",
    "    int_to_vocab = {k:w for k,w in enumerate(vocab)}\n",
    "    vocab_to_int = {w:k for k,w in int_to_vocab.items()}\n",
    "    return int_to_vocab, vocab_to_int\n",
    "\n",
    "words = removepunct(doc2words(doc))\n",
    "vocab = getvocab(words)\n",
    "int_to_vocab, vocab_to_int = vocab_map(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(words, vocab_to_int, batch_size, seq_size):\n",
    "    # generate a Xs and Ys of shape (batchsize * num_batches) * seq_size\n",
    "    word_ints = [vocab_to_int[word] for word in words]\n",
    "    num_batches = int(len(word_ints) / (batch_size * seq_size))\n",
    "    Xs = word_ints[:num_batches*batch_size*seq_size]\n",
    "    Ys = np.zeros_like(Xs)\n",
    "    Ys[:-1] = Xs[1:]\n",
    "    Ys[-1] = Xs[0]\n",
    "    Xs = np.reshape(Xs, (num_batches*batch_size, seq_size))\n",
    "    Ys= np.reshape(Ys, (num_batches*batch_size, seq_size))\n",
    "    \n",
    "    # iterate over rows of Xs and Ys to generate batches\n",
    "    for i in range(0, num_batches*batch_size, batch_size):\n",
    "        yield Xs[i:i+batch_size, :], Ys[i:i+batch_size, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss_and_train_op(net, lr=0.001):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "\n",
    "    return criterion, optimizer\n",
    "    \n",
    "def generate_text(device, net, words, n_vocab, vocab_to_int, int_to_vocab, top_k=5):\n",
    "    net.eval()\n",
    "\n",
    "    state_h, state_c = net.zero_state(1)\n",
    "    state_h = state_h.to(device)\n",
    "    state_c = state_c.to(device)\n",
    "    for w in words:\n",
    "        ix = torch.tensor([[vocab_to_int[w]]]).to(device)\n",
    "        output, (state_h, state_c) = net(ix, (state_h, state_c))\n",
    "    \n",
    "    _, top_ix = torch.topk(output[0], k=top_k)\n",
    "    choices = top_ix.tolist()\n",
    "    choice = np.random.choice(choices[0])\n",
    "\n",
    "    words.append(int_to_vocab[choice])\n",
    "    \n",
    "    for _ in range(100):\n",
    "        ix = torch.tensor([[choice]]).to(device)\n",
    "        output, (state_h, state_c) = net(ix, (state_h, state_c))\n",
    "\n",
    "        _, top_ix = torch.topk(output[0], k=top_k)\n",
    "        choices = top_ix.tolist()\n",
    "        choice = np.random.choice(choices[0])\n",
    "        words.append(int_to_vocab[choice])\n",
    "\n",
    "    print(' '.join(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_rnn(words, vocab_to_int, int_to_vocab, n_vocab):\n",
    "    \n",
    "    # RNN instance\n",
    "    net = SequentialModule(n_vocab, seq_size, embedding_size, lstm_size)\n",
    "    net = net.to(device)\n",
    "    criterion, optimizer = get_loss_and_train_op(net, 0.01)\n",
    "\n",
    "    iteration = 0\n",
    "    \n",
    "    for e in range(10):\n",
    "        batches = get_batches(words, vocab_to_int, batch_size, seq_size)\n",
    "        state_h, state_c = net.zero_state(batch_size)\n",
    "\n",
    "        # Transfer data to GPU\n",
    "        state_h = state_h.to(device)\n",
    "        state_c = state_c.to(device)\n",
    "        for x, y in batches:\n",
    "            iteration += 1\n",
    "\n",
    "            # Tell it we are in training mode\n",
    "            net.train()\n",
    "\n",
    "            # Reset all gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Transfer data to GPU\n",
    "            x = torch.tensor(x).to(device)\n",
    "            y = torch.tensor(y).to(device)\n",
    "\n",
    "            logits, (state_h, state_c) = net(x, (state_h, state_c))\n",
    "            loss = criterion(logits.transpose(1, 2), y)\n",
    "\n",
    "            state_h = state_h.detach()\n",
    "            state_c = state_c.detach()\n",
    "\n",
    "            loss_value = loss.item()\n",
    "\n",
    "            # Perform back-propagation\n",
    "            loss.backward(retain_graph=True)\n",
    "\n",
    "            _ = torch.nn.utils.clip_grad_norm_(net.parameters(), gradients_norm)\n",
    "            \n",
    "            # Update the network's parameters\n",
    "            optimizer.step()\n",
    "\n",
    "            if iteration % 100 == 0:\n",
    "                print('Epoch: {}/{}'.format(e, 10),'Iteration: {}'.format(iteration),'Loss: {}'.format(loss_value))\n",
    "\n",
    "            # if iteration % 1000 == 0:\n",
    "                # predict(device, net, flags.initial_words, n_vocab,vocab_to_int, int_to_vocab, top_k=5)\n",
    "                # torch.save(net.state_dict(),'checkpoint_pt/model-{}.pth'.format(iteration))\n",
    "                \n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/10 Iteration: 100 Loss: 7.231979846954346\n",
      "Epoch: 0/10 Iteration: 200 Loss: 6.964202880859375\n",
      "Epoch: 0/10 Iteration: 300 Loss: 6.74375057220459\n",
      "Epoch: 1/10 Iteration: 400 Loss: 6.750913619995117\n",
      "Epoch: 1/10 Iteration: 500 Loss: 5.9644694328308105\n",
      "Epoch: 1/10 Iteration: 600 Loss: 6.4356184005737305\n",
      "Epoch: 1/10 Iteration: 700 Loss: 6.3372111320495605\n",
      "Epoch: 2/10 Iteration: 800 Loss: 6.194395542144775\n",
      "Epoch: 2/10 Iteration: 900 Loss: 5.7319560050964355\n",
      "Epoch: 2/10 Iteration: 1000 Loss: 5.853903770446777\n",
      "Epoch: 2/10 Iteration: 1100 Loss: 5.552889823913574\n",
      "Epoch: 3/10 Iteration: 1200 Loss: 5.788702011108398\n",
      "Epoch: 3/10 Iteration: 1300 Loss: 5.7780680656433105\n",
      "Epoch: 3/10 Iteration: 1400 Loss: 5.656917095184326\n",
      "Epoch: 3/10 Iteration: 1500 Loss: 5.560693740844727\n",
      "Epoch: 4/10 Iteration: 1600 Loss: 5.155842304229736\n",
      "Epoch: 4/10 Iteration: 1700 Loss: 5.145718097686768\n",
      "Epoch: 4/10 Iteration: 1800 Loss: 5.2600860595703125\n",
      "Epoch: 4/10 Iteration: 1900 Loss: 5.132971286773682\n",
      "Epoch: 5/10 Iteration: 2000 Loss: 5.308545112609863\n",
      "Epoch: 5/10 Iteration: 2100 Loss: 5.34194803237915\n",
      "Epoch: 5/10 Iteration: 2200 Loss: 5.2131571769714355\n",
      "Epoch: 5/10 Iteration: 2300 Loss: 4.572067737579346\n",
      "Epoch: 6/10 Iteration: 2400 Loss: 5.019961833953857\n",
      "Epoch: 6/10 Iteration: 2500 Loss: 4.888901233673096\n",
      "Epoch: 6/10 Iteration: 2600 Loss: 4.823285102844238\n",
      "Epoch: 6/10 Iteration: 2700 Loss: 4.63541316986084\n",
      "Epoch: 7/10 Iteration: 2800 Loss: 4.929285526275635\n",
      "Epoch: 7/10 Iteration: 2900 Loss: 4.619411468505859\n",
      "Epoch: 7/10 Iteration: 3000 Loss: 4.903716564178467\n",
      "Epoch: 7/10 Iteration: 3100 Loss: 4.672457218170166\n",
      "Epoch: 8/10 Iteration: 3200 Loss: 4.460766792297363\n",
      "Epoch: 8/10 Iteration: 3300 Loss: 4.8532915115356445\n",
      "Epoch: 8/10 Iteration: 3400 Loss: 4.691725730895996\n",
      "Epoch: 8/10 Iteration: 3500 Loss: 4.419217109680176\n",
      "Epoch: 9/10 Iteration: 3600 Loss: 4.475979804992676\n",
      "Epoch: 9/10 Iteration: 3700 Loss: 4.758672714233398\n",
      "Epoch: 9/10 Iteration: 3800 Loss: 4.619863033294678\n",
      "Epoch: 9/10 Iteration: 3900 Loss: 4.634983062744141\n"
     ]
    }
   ],
   "source": [
    "rnn_net = train_rnn(words, vocab_to_int, int_to_vocab, len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Today marks you sir what you do thee no of my masters use it was the duke is that I have no screen between thee to my heart LUCENTIO Tranio and my father Pedant I pray Bianca And I pray between your hands the devils musician your name and beat him from the house of the street o it on thee and so I have seen a farmers ground was a very apoplexy pound or the rest that eer that you are choleric Take thy life and therefore shall not have taen him Pedant Sir let the time KATHARINA No my son Lucentio\n"
     ]
    }
   ],
   "source": [
    "generate_text(device, rnn_net, ['Today', 'marks'], len(vocab), vocab_to_int, int_to_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to Generate?\n",
    "\n",
    "- Greedy decoding: Always pick the word with the highest probability \n",
    "\n",
    "- Sampling: Sample a word according to the given distribution\n",
    "\n",
    "- Beam search:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beam Search:\n",
    "\n",
    "Beam search is a strategy used in algorithms that need to make a series of choices from potentially very large sets of options, such as generating sentences word by word in natural language processing.\n",
    "\n",
    "At every step, predicting a different word, could lead the text generated in a completely different path, and most likely there are far too many paths for the model to explore all of them. \n",
    "\n",
    "Beam search helps by keeping track of a limited number of promosing paths (incomplete sentences), at each step, and extends those paths with new words (until the end of the sequence or maximum length is reached).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Greedy Search\n",
    "\n",
    "![title](https://huggingface.co/blog/assets/02_how-to-generate/greedy_search.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting from the word \"The\", the algorithm greedily chooses the next word of highest probability \"nice\", and so on, so that the final generated word sequence is (\"The\", \"nice\", \"woman\") having an overall probability of 0.5 × 0.4 = 0.2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](https://huggingface.co/blog/assets/02_how-to-generate/beam_search.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At time step 1, besides the most likely hypothesis (\"The\", \"nice\"), beam search also keeps track of the second most likely one (\"The\", \"dog\"). At time step 2, beam search finds that the word sequence (\"The\", \"dog\", \"has\") has with 0.36 a higher probability than (\"The\", \"nice\", \"woman\"), which has 0.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Beam Width: 3\n",
      "Generated Text: As the sun set over the horizon, the sky was filled with clouds.\n",
      "\n",
      "\"What's going on here?\"\n",
      "\n",
      "\"I don't know.\"\n",
      "\n",
      "\"What's going on here?\"\n",
      "\n",
      "\"I don't know.\"\n",
      "\n",
      "\"What's going on?\"\n",
      "\n",
      "Beam Width: 5\n",
      "Generated Text: As the sun set over the horizon, the sky was filled with light.\n",
      "\n",
      "\"It's been a long time since I've seen anything like this.\"\n",
      ".\n",
      "\n",
      "Beam Width: 10\n",
      "Generated Text: As the sun set over the horizon, there was a flash of light in the middle of the street.\n",
      "\n",
      "\"What's going on?\" I asked.\n",
      ".\n",
      "\n",
      "Beam Width: 20\n",
      "Generated Text: As the sun set over the horizon, there was nowhere to be found.\n",
      "\n",
      "There was nowhere to hide.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "def generate_text_beam_search(model, tokenizer, prompt_text, beam_width=5, max_length=100):\n",
    "    def beam_search_step(input_ids, cum_log_probs):\n",
    "        outputs = model(input_ids=input_ids)\n",
    "        logits = outputs.logits[:, -1, :]\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        log_probs, next_tokens = torch.topk(probs, beam_width, dim=-1)\n",
    "        return log_probs, next_tokens\n",
    "\n",
    "    input_ids = tokenizer.encode(prompt_text, return_tensors='pt')\n",
    "    beam_candidates = [(input_ids, 0.0, 0)]  # (input_ids, cumulative_log_prob, length)\n",
    "    completed_sequences = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for step in range(max_length):\n",
    "            all_candidates = []\n",
    "            for input_ids, cum_log_prob, length in beam_candidates:\n",
    "                log_probs, next_tokens = beam_search_step(input_ids, cum_log_prob)\n",
    "                for i in range(beam_width):\n",
    "                    new_input_ids = torch.cat([input_ids, next_tokens[:, i].unsqueeze(0)], dim=-1)\n",
    "                    new_cum_log_prob = cum_log_prob + log_probs[:, i].item()\n",
    "                    new_length = length + 1\n",
    "\n",
    "                    if next_tokens[:, i].item() == tokenizer.eos_token_id:\n",
    "                        perplexity = torch.exp(torch.tensor(-new_cum_log_prob) / new_length)\n",
    "                        completed_sequences.append((new_input_ids, perplexity))\n",
    "                    else:\n",
    "                        all_candidates.append((new_input_ids, new_cum_log_prob, new_length))\n",
    "            beam_candidates = sorted(all_candidates, key=lambda x: x[1], reverse=True)[:beam_width]\n",
    "            if len(completed_sequences) >= 1:  # Break if at least one sequence is completed\n",
    "                break\n",
    "    \n",
    "    # beam_candidate_sentences = [tokenizer.decode(candidate[0][0], skip_special_tokens=True) for candidate in beam_candidates]\n",
    "    # Choose the sequence with the lowest perplexity among completed sequences\n",
    "    if completed_sequences:\n",
    "        best_sequence = min(completed_sequences, key=lambda x: x[1])\n",
    "        return tokenizer.decode(best_sequence[0][0], skip_special_tokens=True), best_sequence[1].item()\n",
    "    else:\n",
    "        # If no sequences were completed, return the best of the current candidates\n",
    "        best_sequence = min(beam_candidates, key=lambda x: x[1])\n",
    "        return tokenizer.decode(best_sequence[0][0], skip_special_tokens=True)\n",
    "\n",
    "# Initialization\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2', cache_dir='/projectnb/ds598/projects/xthomas/misc')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2', cache_dir='/projectnb/ds598/projects/xthomas/misc')\n",
    "\n",
    "\n",
    "# Generating text with different beam widths\n",
    "prompt = \"As the sun set over the horizon,\"\n",
    "beam_widths = [3, 5, 10, 20]\n",
    "max_length = 50\n",
    "\n",
    "for width in beam_widths:\n",
    "    print(f\"\\nBeam Width: {width}\")\n",
    "    text, perplexity = generate_text_beam_search(model, tokenizer, prompt, beam_width=width, max_length=max_length)\n",
    "    print(f\"Generated Text: {text}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl4ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
