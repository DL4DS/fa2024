{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set HF datasets cache\n",
    "os.environ[\"HF_HOME\"] = \"/projectnb/ds598/admin/xthomas/sp2024_notebooks/discussion/tmp\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformers and Attention Mechanism\n",
    "\n",
    "![img](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kf871smtQKXAf3dSYOlVPA.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img](https://storrs.io/content/images/2021/08/Screen-Shot-2021-08-07-at-7.51.37-AM.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7sy0KlRgyh3n0M0SH-6xqQ.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img](https://storrs.io/content/images/size/w1600/2021/08/image3--8-.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img](https://storrs.io/content/images/size/w1600/2021/08/image8--1-.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://storrs.io/content/images/size/w1600/2021/08/image7--2-.png\" alt=\"Description of Image\" width=\"800px\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://storrs.io/content/images/size/w1600/2021/08/image4--2-.png\" alt=\"Description of Image\" width=\"800px\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://storrs.io/content/images/size/w1600/2021/08/image6--3-.png\" alt=\"Description of Image\" width=\"800px\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://storrs.io/content/images/size/w1600/2021/08/image5--3-.png\" alt=\"Alt Text\" width=\"800px\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YE0dWWP7uzWIa5zmNN2xdA.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vision-Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Components of a Vision Transformer\n",
    "\n",
    "Since Vision Transformer is based on standard transformer architecture, the only difference is that it is being used for image tasks rather than for text, components used here are almost the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Split the image into image patches    \n",
    "\n",
    "![title](https://cdn.sanity.io/images/vr8gru94/production/7a096efc8f3cc40849ee17a546dc0e685da2dc73-4237x1515.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Linear projection to pacth embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](https://cdn.sanity.io/images/vr8gru94/production/88ec9bd73a8c7c2d0d00098eebe1e1281e8c80e4-2193x1117.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The linear projection layer attempts to transform arrays into vectors while maintaining their “physical dimensions”. Meaning similar image patches should be mapped to similar patch embeddings. Linear projection transforms the input to have the correct dimensions for the transformer model.\n",
    "\n",
    "Each patch is considered as a single token, and the image is flattened into a sequence of tokens.\n",
    "\n",
    "The patches are flattened and linearly transformed. The linear transformation is applied to each patch independently, and the same linear transformation is applied to all patches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Learnable Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One feature introduced to transformers with the popular BERT models was the use of a `[CLS]` (or “classification”) token. The `[CLS]` token was a “special token” prepended to every sentence fed into BERT\n",
    "\n",
    "<img src=\"https://cdn.sanity.io/images/vr8gru94/production/cc1a9b538be26c73a668540350e2485a046c2abb-2024x1309.png\" alt=\"Description of Image\" width=\"800px\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](https://cdn.sanity.io/images/vr8gru94/production/ef2026fc131e20c7fb0b0298ab88d5e365339514-4009x1821.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This `[CLS]` token is converted into a token embedding and passed through several encoding layers.\n",
    "Two things make `[CLS]` embeddings special. First, it does not represent an actual token, meaning it begins as a “blank slate” for each sentence. Second, the final output from the `[CLS]` embedding is used as the input into a classification head during pretraining.\n",
    "Using a “blank slate” token as the sole input to a classification head pushes the transformer to learn to encode a “general representation” of the entire sentence into that embedding. The model must do this to enable accurate classifier predictions.\n",
    "ViT applies the same logic by adding a “learnable embedding”. This learnable embedding is the same as the `[CLS]` token used by BERT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Positional Embeddings\n",
    "\n",
    "Transformers do not have any default mechanism that considers the “order” of token or patch embeddings. Yet, order is essential. In language, the order of words can completely change their meaning.    \n",
    "\n",
    "The same is true for images. If given a jumbled jigsaw set, it’s hard-to-impossible for a person to accurately predict what the complete puzzle represents. This applies to transformers too. We need a way of enabling the model to infer the order or position of the puzzle pieces.\n",
    "We enable order with positional embeddings. For ViT, these positional embeddings are learned vectors with the same dimensionality as our patch embeddings.    \n",
    "|\n",
    "After creating the patch embeddings and prepending the “class” embedding, we sum them all with positional embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: the positional embeddings in ViT's are typically learned, not fixed like in the original transformer model. During training, these embeddings converge into vector spaces where they show high similarity to their neighboring position embeddings — particularly those sharing the same column and row:\n",
    "\n",
    "<img src=\"https://cdn.sanity.io/images/vr8gru94/production/f29a1da461dbb154ce8bb2789962d20f8af65587-1911x1551.png\" alt=\"Description of Image\" width=\"800px\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The visualization indicates that positional embeddings are more similar to their immediate neighbors, particularly those within the same row or column. This makes intuitive sense, as adjacent patches in an image are more likely to be related to each other than patches that are further apart."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vision-Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What Are Vision Language Models?\n",
    "\n",
    "A vision-language model is a fusion of vision and natural language models. It ingests images and their respective textual descriptions as inputs and learns to associate the knowledge from the two modalities. The vision part of the model captures spatial features from the images, while the language model encodes information from the text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](https://cdn.sanity.io/images/vr8gru94/production/a54a2f1fa0aeac03748c09df0fdfbb42aadc96b7-2430x1278.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar text and images will be encoded into a similar vector space. Dissimilar text and images do not share a similar vector space.\n",
    "\n",
    "Both models “speak the same language” by encoding similar concepts in text and images into similar vectors. That means that the text “two dogs running across a frosty field” would output a vector similar to an image of two dogs running across a frosty field."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLIP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](https://cdn.sanity.io/images/vr8gru94/production/539716ea1571e459908c1fdc5a898fea239d8243-2803x1672.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CLIP consists of two models trained in parallel. A 12-layer text transformer for building text embeddings and a ResNet or vision transformer (ViT) for building image embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both these models were trained seperately, and by default, have no understanding of one another. CLIP solves this thanks to image-text contrastive pretraining.     \n",
    "\n",
    "Contrastive pretraining works by taking a (text, image) pair – where the text describes the image – and learning to encode the pairs as closely as possible in vector space.    \n",
    "For this to work well, we also need negative pairs to provide a contrastive comparison. We need positive pairs that should output similar vectors and negative pairs that should output dissimilar vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contrastive Learning\n",
    "\n",
    "a technique that learns data points by understanding their differences. The method computes a similarity score between data instances and aims to minimize contrastive loss. It’s most useful in semi-supervised learning, where only a few labeled samples guide the optimization process to label unseen data points.\n",
    "\n",
    "![title](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7xMtz23e_U1xfqVWYrsevA.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Positive pairs:\n",
    "\n",
    "(T1, I1)\n",
    "(T2, I2)\n",
    "From these, we generate negative pairs by swapping the components:\n",
    "\n",
    "Negative pair 1: (T1, I2)\n",
    "Negative pair 2: (T2, I1)\n",
    "The loss function aims to:\n",
    "\n",
    "Maximize similarity between (T1, I1) and (T2, I2)\n",
    "Minimize similarity between (T1, I2) and (T2, I1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](https://cdn.sanity.io/images/vr8gru94/production/d6868e6dae721512fed8f1287fc9ffe6b6a2cddd-2332x1342.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: A fundamental assumption is that there are no other positive pairs within a single batch. For example, we assume that “two dogs running across a frosty field” is only relevant to the image it is paired with. We assume there are no other texts or images with similar meanings.    \n",
    "\n",
    "This assumption is possible because the datasets used for pretraining are diverse and large enough that the likelihood of two similar pairs appearing in a single batch is negligible. Therefore, rare enough to have a little-to-no negative impact on pretraining performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zero-Shot Image Classification with CLIP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How does CLIP do zero-shot classification?\n",
    "\n",
    "- We know that the image and text encoder creates a 512-dimensional image and text vector that map to the same vector space.\n",
    "Considering this vector space alignment, what if we wrote the dataset classes as text sentences?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a task where we must identify whether a photo contains a car, bird, or cat: we could create and encode three text classes:\n",
    "\"a photo of a car\" -> T_1,  \"a photo of a bird\" -> T_2,  \"a photo of a cat\" -> T_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of these \"classes\" are output from the text encoder as vectors T1, T2, and T3, respectively. Given a photo of a cat, we encode it with the ViT model to create vector I1. When we calculate the similarity of these vectors with cosine similarity, we expect sim(T3, I1) to return the highest score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](https://cdn.sanity.io/images/vr8gru94/production/d9a6ebbc9a2f3334ec57a6b54d90155043c07595-1292x447.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why bother with creating a sentence for each class?\n",
    "\n",
    "- We format the one-word classes into sentences because we expect CLIP saw more sentence-like text during pretraining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/projectnb/ds598/projects/xthomas/.conda/envs/ds598/lib/python3.9/site-packages/datasets/load.py:1454: FutureWarning: The repository for frgfm/imagenette contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/frgfm/imagenette\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['image', 'label'],\n",
       "    num_rows: 3925\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import the imagenette dataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "imagenette = load_dataset(\n",
    "    'frgfm/imagenette',\n",
    "    '320px',\n",
    "    split='validation',\n",
    "    revision=\"4d512db\"\n",
    ")\n",
    "# show dataset info\n",
    "imagenette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0, 1, 2, 3, 4, 5, 6, 7, 8, 9}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check labels in the dataset\n",
    "set(imagenette['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tench',\n",
       " 'English springer',\n",
       " 'cassette player',\n",
       " 'chain saw',\n",
       " 'church',\n",
       " 'French horn',\n",
       " 'garbage truck',\n",
       " 'gas pump',\n",
       " 'golf ball',\n",
       " 'parachute']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# labels names \n",
    "labels = imagenette.info.features['label'].names\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a photo of a tench',\n",
       " 'a photo of a English springer',\n",
       " 'a photo of a cassette player',\n",
       " 'a photo of a chain saw',\n",
       " 'a photo of a church',\n",
       " 'a photo of a French horn',\n",
       " 'a photo of a garbage truck',\n",
       " 'a photo of a gas pump',\n",
       " 'a photo of a golf ball',\n",
       " 'a photo of a parachute']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate sentences\n",
    "clip_labels = [f\"a photo of a {label}\" for label in labels]\n",
    "clip_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialization\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "model_id = \"openai/clip-vit-base-patch32\"\n",
    "\n",
    "processor = CLIPProcessor.from_pretrained(model_id, cache_dir=\"/projectnb/ds598/admin/xthomas/sp2024_notebooks/discussion/tmp\")\n",
    "model = CLIPModel.from_pretrained(model_id, cache_dir=\"/projectnb/ds598/admin/xthomas/sp2024_notebooks/discussion/tmp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([49406,   320,  1125,   539,   320,  1149,   634, 49407])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create label tokens\n",
    "label_tokens = processor(\n",
    "    text=clip_labels,\n",
    "    padding=True,\n",
    "    images=None,\n",
    "    return_tensors='pt'\n",
    ").to(device)\n",
    "\n",
    "label_tokens['input_ids'][0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 512)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# encode tokens to sentence embeddings\n",
    "label_emb = model.get_text_features(**label_tokens)\n",
    "# detach from pytorch gradient computation\n",
    "label_emb = label_emb.detach().cpu().numpy()\n",
    "label_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e140ba03e6c4e0399cd29a3eff91ef0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/123 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# if you have CUDA set it to the active device like this\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# move the model to the device\n",
    "model.to(device)\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "preds = []\n",
    "batch_size = 32\n",
    "\n",
    "for i in tqdm(range(0, len(imagenette), batch_size)):\n",
    "    i_end = min(i + batch_size, len(imagenette))\n",
    "    images = processor(\n",
    "        text=None,\n",
    "        images=imagenette[i:i_end]['image'],\n",
    "        return_tensors='pt'\n",
    "    )['pixel_values'].to(device)\n",
    "    img_emb = model.get_image_features(images)\n",
    "    img_emb = img_emb.detach().cpu().numpy()\n",
    "    scores = np.dot(img_emb, label_emb.T)\n",
    "    preds.extend(np.argmax(scores, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.965859872611465"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_preds = []\n",
    "for i, label in enumerate(imagenette['label']):\n",
    "    if label == preds[i]:\n",
    "        true_preds.append(1)\n",
    "    else:\n",
    "        true_preds.append(0)\n",
    "\n",
    "sum(true_preds) / len(true_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Zero Shot Acc: ~96.58%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Object Localization\n",
    "\n",
    "Prompt: `\"A photo of a cat\"`\n",
    "\n",
    "<img src=\"https://cdn.sanity.io/images/vr8gru94/production/a8b3638cd76b8c92f728f71bb81b62bb584e4beb-1171x1784.png\" width=\"292.75\" height=\"446\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the scores at each window, weight the pixel values by the scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](https://cdn.sanity.io/images/vr8gru94/production/a7a86913765c70dfa415ffbf28bdedaf49a89699-1706x913.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can repeat the same but with the prompt `\"A photo of a butterfly\"` to return:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.sanity.io/images/vr8gru94/production/df6a8d683c8d42ffc3dc8643506a91c4704fa348-894x1328.png\" width=\"223.5\" height=\"332\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.sanity.io/images/vr8gru94/production/c01610aed767d1e42eb01575a71dfacc1d7f7097-409x578.png\" width=\"223.5\" height=\"332\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ref: https://encord.com/blog/vision-language-models-guide/#:~:text=Vision%2Dlanguage%20models%20are%20a,visual%20semantics%20to%20textual%20representations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds598",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
